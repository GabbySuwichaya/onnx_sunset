{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph main_graph (\n",
      "  %input_image[FLOAT, batch_sizex48x64x64]\n",
      "  %input_scalar[FLOAT, batch_sizex16]\n",
      ") initializers (\n",
      "  %conv1.weight[FLOAT, 48x48x3x3]\n",
      "  %conv1.bias[FLOAT, 48]\n",
      "  %batchnorm1.weight[FLOAT, 48]\n",
      "  %batchnorm1.bias[FLOAT, 48]\n",
      "  %batchnorm1.running_mean[FLOAT, 48]\n",
      "  %batchnorm1.running_var[FLOAT, 48]\n",
      "  %conv2.weight[FLOAT, 96x48x3x3]\n",
      "  %conv2.bias[FLOAT, 96]\n",
      "  %batchnorm2.weight[FLOAT, 96]\n",
      "  %batchnorm2.bias[FLOAT, 96]\n",
      "  %batchnorm2.running_mean[FLOAT, 96]\n",
      "  %batchnorm2.running_var[FLOAT, 96]\n",
      "  %concat.weight[FLOAT, 1024x24592]\n",
      "  %concat.bias[FLOAT, 1024]\n",
      "  %dense1.weight[FLOAT, 1024x1024]\n",
      "  %dense1.bias[FLOAT, 1024]\n",
      "  %dense2.weight[FLOAT, 15x1024]\n",
      "  %dense2.bias[FLOAT, 15]\n",
      "  %dense3.weight[FLOAT, 15x15]\n",
      "  %dense3.bias[FLOAT, 15]\n",
      ") {\n",
      "  %/conv1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%input_image, %conv1.weight, %conv1.bias)\n",
      "  %/relu/Relu_output_0 = Relu(%/conv1/Conv_output_0)\n",
      "  %/batchnorm1/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142, training_mode = 0](%/relu/Relu_output_0, %batchnorm1.weight, %batchnorm1.bias, %batchnorm1.running_mean, %batchnorm1.running_var)\n",
      "  %/maxpool1/MaxPool_output_0 = MaxPool[ceil_mode = 0, dilations = [1, 1], kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]](%/batchnorm1/BatchNormalization_output_0)\n",
      "  %/conv2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/maxpool1/MaxPool_output_0, %conv2.weight, %conv2.bias)\n",
      "  %/relu_1/Relu_output_0 = Relu(%/conv2/Conv_output_0)\n",
      "  %/batchnorm2/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142, training_mode = 0](%/relu_1/Relu_output_0, %batchnorm2.weight, %batchnorm2.bias, %batchnorm2.running_mean, %batchnorm2.running_var)\n",
      "  %/maxpool2/MaxPool_output_0 = MaxPool[ceil_mode = 0, dilations = [1, 1], kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]](%/batchnorm2/BatchNormalization_output_0)\n",
      "  %/flatten/Flatten_output_0 = Flatten[axis = 1](%/maxpool2/MaxPool_output_0)\n",
      "  %/Concat_output_0 = Concat[axis = 1](%/flatten/Flatten_output_0, %input_scalar)\n",
      "  %/concat/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/Concat_output_0, %concat.weight, %concat.bias)\n",
      "  %/relu_2/Relu_output_0 = Relu(%/concat/Gemm_output_0)\n",
      "  %/dense1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/relu_2/Relu_output_0, %dense1.weight, %dense1.bias)\n",
      "  %/relu_3/Relu_output_0 = Relu(%/dense1/Gemm_output_0)\n",
      "  %/dense2/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/relu_3/Relu_output_0, %dense2.weight, %dense2.bias)\n",
      "  %output = Gemm[alpha = 1, beta = 1, transB = 1](%/dense2/Gemm_output_0, %dense3.weight, %dense3.bias)\n",
      "  return %output\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import yaml \n",
    "import os\n",
    "\n",
    "device      = \"cuda\"\n",
    "batch_size  = 64\n",
    "config_path = \"configs/sunset_configs.yaml\"\n",
    "\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    configs = yaml.safe_load(file)\n",
    "\n",
    "explorted_file = os.path.join(configs[\"model_dir\"], \"sunset_model.onnx\")\n",
    "onnx_model = onnx.load(explorted_file) \n",
    "\n",
    "onnx.checker.check_model(onnx_model) \n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'sunset_onnex_trained_torch/sunset_model.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netron\n",
    "# visualize the pre-trained model\n",
    "netron.start(explorted_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaap/anaconda3/envs/onnx_sunset/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_validation.py:113: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info.\n",
      "  warnings.warn(\"WARNING: failed to get cudart_version from onnxruntime build info.\")\n",
      "/media/HDD1/Projects/CU/EE_SunshineX/onnx_sunset/myonnxutils/onnx_utils.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_csv_data[\"Datetime\"] = pd.to_datetime(selected_csv_data[\"Datetime\"])\n",
      "image transformation: 100%|██████████| 14290/14290 [01:32<00:00, 154.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stacked samples: 13360\n",
      "For [Test] mode: the number of stacked samples: 668\n",
      "               the number of batches: 83\n"
     ]
    }
   ],
   "source": [
    "from dataloader import sirta_dataset\n",
    "\n",
    "batch_size   = 8\n",
    "test_dataset = sirta_dataset( mode = \"Test\",\n",
    "                                irrad_path  = \"sirta_data/2023\",\n",
    "                                image_path  = \"sirta_data/2023/images\",\n",
    "                                seq_length  = 16,\n",
    "                                pred_length = 15,\n",
    "                                image_size  = 64,\n",
    "                                batch_size  = batch_size,\n",
    "                                training_index_file = \"sirta_data/2023/training.txt\",\n",
    "                                validate_index_file = \"sirta_data/2023/validate.txt\",\n",
    "                                testing_index_file  = \"sirta_data/2023/testing.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:03<00:00, 23.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== First RMSE: 56.9100, Last RMSE 73.0901, Batch RMSE 63.8340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from myonnxutils.onnx_utils import mean_square_error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "session = InferenceSession(os.path.join(configs[\"model_dir\"], \"sunset_model.onnx\"),providers=['CPUExecutionProvider'])\n",
    "\n",
    "first_rmse_list = []\n",
    "last_rmse_list  = [] \n",
    "batch_rmse_list = []\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True) \n",
    "pbar = tqdm.tqdm(test_loader)\n",
    "for _, data_batch in enumerate(pbar): \n",
    "\n",
    "    input_index       = data_batch[0]\n",
    "    input_iclr        = data_batch[1]\n",
    "    input_skyimage    = data_batch[2]\n",
    "    output_irr        = data_batch[3] \n",
    "\n",
    "    input_name1  = session.get_inputs()[0].name\n",
    "    input_name2  = session.get_inputs()[1].name\n",
    "    output_name  = session.get_outputs()[0].name \n",
    "\n",
    "\n",
    "    pred_irradiance_list_of_batches  = session.run(output_names=[output_name], input_feed={input_name1: input_skyimage.float().numpy(), input_name2: input_iclr.float().numpy() })\n",
    "\n",
    "    \n",
    "    first_rmse , last_rmse, batch_rmse = mean_square_error(np.concat(pred_irradiance_list_of_batches, axis=0), output_irr.float().numpy())\n",
    "\n",
    "    first_rmse_list.append(first_rmse)\n",
    "    last_rmse_list.append(last_rmse)\n",
    "    batch_rmse_list.append(batch_rmse)\n",
    " \n",
    " \n",
    "print(f'========== First RMSE: {sum(first_rmse_list)/len(first_rmse_list):.4f}, Last RMSE {sum(last_rmse_list)/len(last_rmse_list):.4f}, Batch RMSE {sum(batch_rmse_list)/len(batch_rmse_list):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx_sunset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
