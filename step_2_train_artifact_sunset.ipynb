{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaap/anaconda3/envs/onnx_sunset/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_validation.py:113: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info.\n",
      "  warnings.warn(\"WARNING: failed to get cudart_version from onnxruntime build info.\")\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "from onnxruntime.training.api import CheckpointState, Module, Optimizer\n",
    "from onnxruntime.training import artifacts  \n",
    "import numpy as np \n",
    "import onnx \n",
    "import netron \n",
    "import tqdm\n",
    "\n",
    "# Load module \n",
    "import yaml \n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configs and the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device      = \"cuda\"\n",
    "batch_size  = 64\n",
    "config_path = \"configs/sunset_configs.yaml\"\n",
    "\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph main_graph (\n",
      "  %input_image[FLOAT, batch_sizex48x64x64]\n",
      "  %input_scalar[FLOAT, batch_sizex16]\n",
      ") initializers (\n",
      "  %conv1.weight[FLOAT, 48x48x3x3]\n",
      "  %conv1.bias[FLOAT, 48]\n",
      "  %batchnorm1.weight[FLOAT, 48]\n",
      "  %batchnorm1.bias[FLOAT, 48]\n",
      "  %batchnorm1.running_mean[FLOAT, 48]\n",
      "  %batchnorm1.running_var[FLOAT, 48]\n",
      "  %conv2.weight[FLOAT, 96x48x3x3]\n",
      "  %conv2.bias[FLOAT, 96]\n",
      "  %batchnorm2.weight[FLOAT, 96]\n",
      "  %batchnorm2.bias[FLOAT, 96]\n",
      "  %batchnorm2.running_mean[FLOAT, 96]\n",
      "  %batchnorm2.running_var[FLOAT, 96]\n",
      "  %concat.weight[FLOAT, 1024x24592]\n",
      "  %concat.bias[FLOAT, 1024]\n",
      "  %dense1.weight[FLOAT, 1024x1024]\n",
      "  %dense1.bias[FLOAT, 1024]\n",
      "  %dense2.weight[FLOAT, 15x1024]\n",
      "  %dense2.bias[FLOAT, 15]\n",
      "  %dense3.weight[FLOAT, 15x15]\n",
      "  %dense3.bias[FLOAT, 15]\n",
      ") {\n",
      "  %/conv1/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%input_image, %conv1.weight, %conv1.bias)\n",
      "  %/relu/Relu_output_0 = Relu(%/conv1/Conv_output_0)\n",
      "  %/batchnorm1/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142, training_mode = 0](%/relu/Relu_output_0, %batchnorm1.weight, %batchnorm1.bias, %batchnorm1.running_mean, %batchnorm1.running_var)\n",
      "  %/maxpool1/MaxPool_output_0 = MaxPool[ceil_mode = 0, dilations = [1, 1], kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]](%/batchnorm1/BatchNormalization_output_0)\n",
      "  %/conv2/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/maxpool1/MaxPool_output_0, %conv2.weight, %conv2.bias)\n",
      "  %/relu_1/Relu_output_0 = Relu(%/conv2/Conv_output_0)\n",
      "  %/batchnorm2/BatchNormalization_output_0 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142, training_mode = 0](%/relu_1/Relu_output_0, %batchnorm2.weight, %batchnorm2.bias, %batchnorm2.running_mean, %batchnorm2.running_var)\n",
      "  %/maxpool2/MaxPool_output_0 = MaxPool[ceil_mode = 0, dilations = [1, 1], kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]](%/batchnorm2/BatchNormalization_output_0)\n",
      "  %/flatten/Flatten_output_0 = Flatten[axis = 1](%/maxpool2/MaxPool_output_0)\n",
      "  %/Concat_output_0 = Concat[axis = 1](%/flatten/Flatten_output_0, %input_scalar)\n",
      "  %/concat/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/Concat_output_0, %concat.weight, %concat.bias)\n",
      "  %/relu_2/Relu_output_0 = Relu(%/concat/Gemm_output_0)\n",
      "  %/dense1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/relu_2/Relu_output_0, %dense1.weight, %dense1.bias)\n",
      "  %/relu_3/Relu_output_0 = Relu(%/dense1/Gemm_output_0)\n",
      "  %/dense2/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/relu_3/Relu_output_0, %dense2.weight, %dense2.bias)\n",
      "  %output = Gemm[alpha = 1, beta = 1, transB = 1](%/dense2/Gemm_output_0, %dense3.weight, %dense3.bias)\n",
      "  return %output\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(os.path.join(configs[\"model_dir\"], \"sunset_model.onnx\")) \n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1.weight',\n",
       " 'conv1.bias',\n",
       " 'batchnorm1.weight',\n",
       " 'batchnorm1.bias',\n",
       " 'conv2.weight',\n",
       " 'conv2.bias',\n",
       " 'batchnorm2.weight',\n",
       " 'batchnorm2.bias',\n",
       " 'concat.weight',\n",
       " 'concat.bias',\n",
       " 'dense1.weight',\n",
       " 'dense1.bias',\n",
       " 'dense2.weight',\n",
       " 'dense2.bias',\n",
       " 'dense3.weight',\n",
       " 'dense3.bias']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(configs[\"model_dir\"], 'param_names.txt'), 'r') as f:\n",
    "    list_text      = f.readlines()   # read into a list\n",
    "    all_parameters = [list_text[i].rstrip('\\n') for i in range(len(list_text))]\n",
    "all_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_params = [all_parameters[i] for i in range(10)]\n",
    "requires_grad = [all_parameters[i] for i in range(10, len(all_parameters))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1.weight',\n",
       " 'conv1.bias',\n",
       " 'batchnorm1.weight',\n",
       " 'batchnorm1.bias',\n",
       " 'conv2.weight',\n",
       " 'conv2.bias',\n",
       " 'batchnorm2.weight',\n",
       " 'batchnorm2.bias',\n",
       " 'concat.weight',\n",
       " 'concat.bias']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozen_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense1.weight',\n",
       " 'dense1.bias',\n",
       " 'dense2.weight',\n",
       " 'dense2.bias',\n",
       " 'dense3.weight',\n",
       " 'dense3.bias']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offline artifacts generation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 15:15:40.006756182 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ConstantSharing modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006774436 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer LayerNormFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006791874 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer CommonSubexpressionElimination modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006796084 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer GeluFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006799979 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer SimplifiedLayerNormFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006805045 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer FastGeluFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006808778 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer QuickGeluFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006812429 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer SoftmaxCrossEntropyLossInternalFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006817232 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer GatherSliceToSplitFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006820815 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer GatherToSliceFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006824406 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer QDQFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006827990 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer BiasGeluFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006833772 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer IsInfReduceSumFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006837337 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ScaledSumFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006841850 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ConstantFolding modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006845489 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ReshapeFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006848928 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ShapeOptimizer modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006855025 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ConcatSliceElimination modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006870301 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer PropagateCastOps modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006874081 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer CastSceLossFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006892092 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer Level1_RuleBasedTransformer modified: 1 with status: OK\n",
      "2025-04-07 15:15:40.006970043 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ConstantSharing modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006974129 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer LayerNormFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006990685 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer CommonSubexpressionElimination modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006994283 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer GeluFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.006997761 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer SimplifiedLayerNormFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007001669 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer FastGeluFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007005029 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer QuickGeluFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007008357 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer SoftmaxCrossEntropyLossInternalFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007012783 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer GatherSliceToSplitFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007016147 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer GatherToSliceFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007019423 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer QDQFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007022663 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer BiasGeluFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007025900 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer IsInfReduceSumFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007029109 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ScaledSumFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007033186 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ConstantFolding modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007036415 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ReshapeFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007039613 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ShapeOptimizer modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007043008 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer ConcatSliceElimination modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007052277 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer PropagateCastOps modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007055739 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer CastSceLossFusion modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007059738 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer Level1_RuleBasedTransformer modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007065694 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer Level2_RuleBasedTransformer modified: 1 with status: OK\n",
      "2025-04-07 15:15:40.007129864 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer Level2_RuleBasedTransformer modified: 0 with status: OK\n",
      "2025-04-07 15:15:40.007518117 [I:onnxruntime:Default, graph_transformer.cc:15 Apply] GraphTransformer pre_training_rule_based_graph_transformer modified: 0 with status: OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.999999747378752e-06\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(configs[\"artifacts_dir\"], exist_ok = True)\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    loss=artifacts.LossType.MSELoss,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    artifact_directory=configs[\"artifacts_dir\"],\n",
    "    additional_output_names=[\"output\"])\n",
    "\n",
    "\n",
    "checkpoint_path    = os.path.join(configs[\"artifacts_dir\"], \"checkpoint\")\n",
    "train_model_path   = os.path.join(configs[\"artifacts_dir\"], \"training_model.onnx\")\n",
    "eval_model_path    = os.path.join(configs[\"artifacts_dir\"], \"eval_model.onnx\")\n",
    "optimizer_path     = os.path.join(configs[\"artifacts_dir\"], \"optimizer_model.onnx\")\n",
    "\n",
    "\n",
    "# load checkpoints.\n",
    "state     = CheckpointState.load_checkpoint(checkpoint_path)\n",
    "\n",
    "# Create module.\n",
    "model     = Module(train_model_path, state, eval_model_path )\n",
    "\n",
    "# Create optimizer.\n",
    "optimizer = Optimizer(optimizer_path, model)\n",
    "optimizer.set_learning_rate(configs[\"learning_rate\"])\n",
    "\n",
    "print(optimizer.get_learning_rate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'sunset_onnex/artifacts/eval_model.onnx' at http://localhost:8081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8081)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the pre-trained model\n",
    "netron.start(os.path.join(configs[\"artifacts_dir\"], \"eval_model.onnx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script generates the list of index for training/validation/testing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\nimport random\\n\\ndata_path   = \"sirta_data/2023\"\\nnum_samples = 13360\\nseq = np.arange(num_samples).tolist()\\n\\ntraining_samples = int(np.floor(0.90*num_samples))\\ntraining_index_list = random.sample(seq, training_samples)\\n\\nseq_rm_training     = list(set(seq) - set(training_index_list))  \\nvalidating_samples  = int(np.floor(0.05*num_samples))\\nvalidate_index_list = random.sample(seq_rm_training, validating_samples)\\n\\ntesting_index_list = list(set(seq_rm_training) - set(validate_index_list))    \\n\\nwith open(os.path.join(data_path, \"training.txt\"), \\'w\\') as file:\\n    for index, item in enumerate(training_index_list):\\n        if index == len(training_index_list)-1:\\n            file.write(str(item) )\\n        else:\\n            file.write(str(item) + \\'\\n\\')\\n\\nwith open(os.path.join(data_path, \"validate.txt\"), \\'w\\') as file: \\n    for index, item in enumerate(validate_index_list):\\n        if index == len(validate_index_list)-1:\\n            file.write(str(item) )\\n        else:\\n            file.write(str(item) + \\'\\n\\')\\n\\nwith open(os.path.join(data_path, \"testing.txt\"), \\'w\\') as file:\\n    for index, item in enumerate(testing_index_list): \\n        if index == len(testing_index_list)-1:\\n            file.write(str(item) )\\n        else:\\n            file.write(str(item) + \\'\\n\\') \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "The following script generates the list of index for training/validation/testing. \n",
    "\n",
    "However, we have already generated them for you in 'sirta_data/2023',\n",
    "and set the number of samples to 13360, which is the total samples for month 05 in year 2023.\n",
    "\n",
    "To generate the new lists, you need to check the number of samples, and adjust the parameter accordingly.  \n",
    "Then, just uncomment the following 'Python' code, so that you can generate the file. \n",
    "''' \n",
    "\n",
    "'''  \n",
    "import random\n",
    "\n",
    "data_path   = \"sirta_data/2023\"\n",
    "num_samples = 13360\n",
    "seq = np.arange(num_samples).tolist()\n",
    "\n",
    "training_samples = int(np.floor(0.90*num_samples))\n",
    "training_index_list = random.sample(seq, training_samples)\n",
    "\n",
    "seq_rm_training     = list(set(seq) - set(training_index_list))  \n",
    "validating_samples  = int(np.floor(0.05*num_samples))\n",
    "validate_index_list = random.sample(seq_rm_training, validating_samples)\n",
    "\n",
    "testing_index_list = list(set(seq_rm_training) - set(validate_index_list))    \n",
    "\n",
    "with open(os.path.join(data_path, \"training.txt\"), 'w') as file:\n",
    "    for index, item in enumerate(training_index_list):\n",
    "        if index == len(training_index_list)-1:\n",
    "            file.write(str(item) )\n",
    "        else:\n",
    "            file.write(str(item) + '\\n')\n",
    "\n",
    "with open(os.path.join(data_path, \"validate.txt\"), 'w') as file: \n",
    "    for index, item in enumerate(validate_index_list):\n",
    "        if index == len(validate_index_list)-1:\n",
    "            file.write(str(item) )\n",
    "        else:\n",
    "            file.write(str(item) + '\\n')\n",
    "\n",
    "with open(os.path.join(data_path, \"testing.txt\"), 'w') as file:\n",
    "    for index, item in enumerate(testing_index_list): \n",
    "        if index == len(testing_index_list)-1:\n",
    "            file.write(str(item) )\n",
    "        else:\n",
    "            file.write(str(item) + '\\n') \n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script sets up the data loader for fine-tunning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD1/Projects/CU/EE_SunshineX/onnx_sunset/myonnxutils/onnx_utils.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_csv_data[\"Datetime\"] = pd.to_datetime(selected_csv_data[\"Datetime\"])\n",
      "image transformation: 100%|██████████| 14290/14290 [03:22<00:00, 70.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stacked samples: 13360\n",
      "For [Train] mode: the number of stacked samples: 12024\n",
      "               the number of batches: 187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD1/Projects/CU/EE_SunshineX/onnx_sunset/myonnxutils/onnx_utils.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_csv_data[\"Datetime\"] = pd.to_datetime(selected_csv_data[\"Datetime\"])\n",
      "image transformation: 100%|██████████| 14290/14290 [01:33<00:00, 152.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stacked samples: 13360\n",
      "For [Valid] mode: the number of stacked samples: 668\n",
      "               the number of batches: 10\n"
     ]
    }
   ],
   "source": [
    "from dataloader import sirta_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "training_dataset = sirta_dataset( mode = \"Train\",\n",
    "                                irrad_path  = \"sirta_data/2023\",\n",
    "                                image_path  = \"sirta_data/2023/images\",\n",
    "                                seq_length  = 16,\n",
    "                                pred_length = 15,\n",
    "                                image_size  = 64,\n",
    "                                batch_size  = batch_size,\n",
    "                                training_index_file = \"sirta_data/2023/training.txt\",\n",
    "                                validate_index_file = \"sirta_data/2023/validate.txt\",\n",
    "                                testing_index_file  = \"sirta_data/2023/testing.txt\") \n",
    "\n",
    "valid_dataset = sirta_dataset( mode = \"Valid\",\n",
    "                                irrad_path  = \"sirta_data/2023\",\n",
    "                                image_path  = \"sirta_data/2023/images\",\n",
    "                                seq_length  = 16,\n",
    "                                pred_length = 15,\n",
    "                                image_size  = 64,\n",
    "                                batch_size  = batch_size,\n",
    "                                training_index_file = \"sirta_data/2023/training.txt\",\n",
    "                                validate_index_file = \"sirta_data/2023/validate.txt\",\n",
    "                                testing_index_file  = \"sirta_data/2023/testing.txt\") \n",
    "    \n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True)  \n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6185.8691: 100%|██████████| 187/187 [00:17<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 6185.8691, First RMSE: 70.3813, Last RMSE 87.3075, Batch RMSE 76.4005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 5608.9600: 100%|██████████| 10/10 [00:00<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Valid Loss: 5608.9600, First RMSE: 64.3839, Last RMSE 94.0798, Batch RMSE 72.8417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6174.2090: 100%|██████████| 187/187 [00:17<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 6174.2090, First RMSE: 70.3453, Last RMSE 87.6623, Batch RMSE 76.3585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 5592.9224: 100%|██████████| 10/10 [00:00<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Valid Loss: 5592.9224, First RMSE: 64.0577, Last RMSE 93.8601, Batch RMSE 72.7291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6158.6470: 100%|██████████| 187/187 [00:17<00:00, 10.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 6158.6470, First RMSE: 70.2371, Last RMSE 87.6622, Batch RMSE 76.4045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 5602.6025: 100%|██████████| 10/10 [00:00<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Valid Loss: 5602.6025, First RMSE: 64.2233, Last RMSE 94.2004, Batch RMSE 72.7791\n",
      "EarlyStopping counter: 1 out of 3\n",
      "------- Adjust LR: 0.0000 ==> 4.999999873689376e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6161.6851: 100%|██████████| 187/187 [00:16<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 6161.6851, First RMSE: 69.8727, Last RMSE 87.6479, Batch RMSE 76.3507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 5595.8203: 100%|██████████| 10/10 [00:00<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Valid Loss: 5595.8203, First RMSE: 63.9957, Last RMSE 94.0792, Batch RMSE 72.7190\n",
      "------- Adjust LR: 0.0000 ==> 2.499999936844688e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6147.9404: 100%|██████████| 187/187 [00:17<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 6147.9404, First RMSE: 70.4702, Last RMSE 87.7631, Batch RMSE 76.3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 5594.2822: 100%|██████████| 10/10 [00:00<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Valid Loss: 5594.2822, First RMSE: 63.9750, Last RMSE 93.7443, Batch RMSE 72.7109\n",
      "------- Adjust LR: 0.0000 ==> 1.249999968422344e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6134.5864: 100%|██████████| 187/187 [00:17<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 6134.5864, First RMSE: 69.6849, Last RMSE 87.6275, Batch RMSE 76.1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 5589.0347: 100%|██████████| 10/10 [00:00<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Valid Loss: 5589.0347, First RMSE: 63.9963, Last RMSE 93.8464, Batch RMSE 72.6903\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6144.7988: 100%|██████████| 187/187 [00:17<00:00, 10.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 6144.7988, First RMSE: 69.5998, Last RMSE 87.6347, Batch RMSE 76.2262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 5588.9639: 100%|██████████| 10/10 [00:00<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Valid Loss: 5588.9639, First RMSE: 64.0314, Last RMSE 93.8264, Batch RMSE 72.6811\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6134.9136: 100%|██████████| 187/187 [00:17<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 6134.9136, First RMSE: 69.6063, Last RMSE 87.5390, Batch RMSE 76.0798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 5588.3311: 100%|██████████| 10/10 [00:00<00:00, 10.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Valid Loss: 5588.3311, First RMSE: 63.9824, Last RMSE 93.9167, Batch RMSE 72.6790\n",
      "EarlyStopping counter: 3 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from myonnxutils.onnx_utils import EarlyStopping, AdjustLR, mean_square_error\n",
    "\n",
    "early_stopper = EarlyStopping(patience=configs[\"early_stopper_patience\"])\n",
    "adjustlr      = AdjustLR(patience=1)\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    first_rmse_list = []\n",
    "    last_rmse_list = []\n",
    "    batch_rmse_list = []\n",
    "    pbar = tqdm.tqdm(train_loader)\n",
    "    for _, data_batch in enumerate(pbar): \n",
    "        input_index       = data_batch[0]\n",
    "        input_iclr        = data_batch[1]\n",
    "        input_skyimage    = data_batch[2]\n",
    "        output_irr        = data_batch[3] \n",
    "    \n",
    "        forward_inputs = [input_skyimage.float().numpy(), input_iclr.float().numpy(), output_irr.float().numpy()]\n",
    "        train_loss, pred_irradiance = model(*forward_inputs)\n",
    "        optimizer.step()\n",
    "        model.lazy_reset_grad()\n",
    "\n",
    "        losses.append(train_loss)\n",
    "        first_rmse , last_rmse, batch_rmse = mean_square_error(pred_irradiance, output_irr.float().numpy())\n",
    "        first_rmse_list.append(first_rmse)\n",
    "        last_rmse_list.append(last_rmse)\n",
    "        batch_rmse_list.append(batch_rmse)\n",
    "        pbar.set_description(\"Loss: %.4f\" % (sum(losses)/len(losses)))\n",
    "\n",
    "    train_loss = sum(losses)/len(losses)\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {sum(losses)/len(losses):.4f}, First RMSE: {sum(first_rmse_list)/len(first_rmse_list):.4f}, Last RMSE {sum(last_rmse_list)/len(last_rmse_list):.4f}, Batch RMSE {sum(batch_rmse_list)/len(batch_rmse_list):.4f}')\n",
    " \n",
    " \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    first_rmse_list = []\n",
    "    last_rmse_list = [] \n",
    "    batch_rmse_list = [] \n",
    "\n",
    "    vpbar = tqdm.tqdm(valid_loader)\n",
    "    for _, data_batch in enumerate(vpbar): \n",
    "        input_index       = data_batch[0]\n",
    "        input_iclr        = data_batch[1]\n",
    "        input_skyimage    = data_batch[2]\n",
    "        output_irr        = data_batch[3] \n",
    "\n",
    "    \n",
    "        forward_inputs = [input_skyimage.float().numpy(), input_iclr.float().numpy(), output_irr.float().numpy()]\n",
    "        test_loss, pred_irradiance = model(*forward_inputs)  \n",
    "        first_rmse , last_rmse, batch_rmse = mean_square_error(pred_irradiance, output_irr.float().numpy())\n",
    "        first_rmse_list.append(first_rmse)\n",
    "        last_rmse_list.append(last_rmse)\n",
    "        batch_rmse_list.append(batch_rmse)\n",
    "        losses.append(test_loss)\n",
    "        vpbar.set_description(\"Loss: %.4f\" % (sum(losses)/len(losses)))\n",
    "\n",
    "    valid_loss = sum(losses)/len(losses)\n",
    "\n",
    "   # metrics = metric.compute()\n",
    "    print(f'========== Valid Loss: {valid_loss:.4f}, First RMSE: {sum(first_rmse_list)/len(first_rmse_list):.4f}, Last RMSE {sum(last_rmse_list)/len(last_rmse_list):.4f}, Batch RMSE {sum(batch_rmse_list)/len(batch_rmse_list):.4f}')\n",
    "\n",
    "    saved_params = {\n",
    "        \"model\": model \n",
    "    }\n",
    "    early_stopper(sum(first_rmse_list)/len(first_rmse_list), saved_params, configs)\n",
    "    adjustlr(valid_loss)\n",
    "\n",
    "    if adjustlr.do_adjust: \n",
    "        current_lr = optimizer.get_learning_rate()\n",
    "        optimizer.set_learning_rate(0.5*current_lr) \n",
    "        print(f'------- Adjust LR: {current_lr:.4f} ==> {0.5*current_lr}')\n",
    "        \n",
    "\n",
    "    if early_stopper.early_stop:\n",
    "        break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_device',\n",
       " '_device_type',\n",
       " '_model',\n",
       " '_session_options',\n",
       " '_state',\n",
       " 'copy_buffer_to_parameters',\n",
       " 'eval',\n",
       " 'export_model_for_inferencing',\n",
       " 'get_contiguous_parameters',\n",
       " 'get_parameters_size',\n",
       " 'input_names',\n",
       " 'lazy_reset_grad',\n",
       " 'output_names',\n",
       " 'train',\n",
       " 'training']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the fine-tunned model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD1/Projects/CU/EE_SunshineX/onnx_sunset/myonnxutils/onnx_utils.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_csv_data[\"Datetime\"] = pd.to_datetime(selected_csv_data[\"Datetime\"])\n",
      "image transformation: 100%|██████████| 14290/14290 [01:34<00:00, 150.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stacked samples: 13360\n",
      "For [Test] mode: the number of stacked samples: 668\n",
      "               the number of batches: 41\n"
     ]
    }
   ],
   "source": [
    "batch_size   = 16\n",
    "test_dataset = sirta_dataset( mode = \"Test\",\n",
    "                                irrad_path  = \"sirta_data/2023\",\n",
    "                                image_path  = \"sirta_data/2023/images\",\n",
    "                                seq_length  = 16,\n",
    "                                pred_length = 15,\n",
    "                                image_size  = 64,\n",
    "                                batch_size  = batch_size,\n",
    "                                training_index_file = \"sirta_data/2023/training.txt\",\n",
    "                                validate_index_file = \"sirta_data/2023/validate.txt\",\n",
    "                                testing_index_file  = \"sirta_data/2023/testing.txt\") \n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/41 [00:00<00:05,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:02<00:00, 19.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== First RMSE: 60.9196, Last RMSE 79.4338, Batch RMSE 69.6282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime import InferenceSession \n",
    "\n",
    "first_rmse_list = []\n",
    "last_rmse_list  = [] \n",
    "batch_rmse_list = []\n",
    " \n",
    "pbar = tqdm.tqdm(test_loader)\n",
    "\n",
    "print(onnxruntime.get_available_providers()) \n",
    "\n",
    "session = InferenceSession(os.path.join(configs[\"artifacts_dir\"], 'inference_model.onnx'),providers=['CPUExecutionProvider'])\n",
    "\n",
    "input_name1  = session.get_inputs()[0].name\n",
    "input_name2  = session.get_inputs()[1].name\n",
    "output_name  = session.get_outputs()[0].name  \n",
    "\n",
    "for _, data_batch in enumerate(pbar): \n",
    "\n",
    "    input_index       = data_batch[0]\n",
    "    input_iclr        = data_batch[1]\n",
    "    input_skyimage    = data_batch[2]\n",
    "    output_irr        = data_batch[3] \n",
    "\n",
    "\n",
    "\n",
    "    pred_irradiance_list_of_batches  = session.run(output_names=[output_name], input_feed={input_name1: input_skyimage.float().numpy(), input_name2: input_iclr.float().numpy() })\n",
    "\n",
    "    \n",
    "    first_rmse , last_rmse, batch_rmse = mean_square_error(np.concat(pred_irradiance_list_of_batches, axis=0), output_irr.float().numpy())\n",
    "\n",
    "    first_rmse_list.append(first_rmse)\n",
    "    last_rmse_list.append(last_rmse)\n",
    "    batch_rmse_list.append(batch_rmse)\n",
    " \n",
    " \n",
    "print(f'========== First RMSE: {sum(first_rmse_list)/len(first_rmse_list):.4f}, Last RMSE {sum(last_rmse_list)/len(last_rmse_list):.4f}, Batch RMSE {sum(batch_rmse_list)/len(batch_rmse_list):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx_sunset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
